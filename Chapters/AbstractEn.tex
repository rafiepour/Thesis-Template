\AbstractEn{
	With the increasing popularity of smartphones, adoption of dialogue-system-based tools have seen significant growth.  Natural language understanding is a key component of dialogue systems, as it can be a major bottleneck in the dialogue system workflow. Intent-detection and slot-filling are the two main tasks in natural language understanding. Recurrent Neural Networks have been extensively explored to improve these tasks, but they have well-established weaknesses, namely gradient vanishing and high training time. Recently, Transformer was introduced to rectify said flaws. Moreover, we observed that only a few models further encode the pre-trained language model's output. In this thesis, CTran is propose. CTran is a novel encoder-decoder CNN-Transformer-based architecture designed for intent-detection and slot-filling. In the encoder, BERT is utilized as a word embedding. Then, several convolutional layers with different kernel sizes are used, which are then transposed and concatenated. In the last part of the encoder, stacked Transformer encoders are used to provide final output of the encoder. For the intent-detection decoder, self-attention is utilized followed by a linear layer. In the slot-filling decoder, the aligned Transformer decoder is introduced, which utilizes a heuristical diagonal mask. The diagonal mask provides access to encoder positions which correspond to each target token, and hides other positions. Finally, to evaluate the performance of the proposed model, it is applied on ATIS and SNIPS. The results show that CTran achieve better results than the current state-of-the-art in slot-filling on both datasets.  Furthermore, two strategies, meaning language model as word embedding, and language model as an encoder, are compared. The results show that language model as word embedding strategy yields a better result.
}

\KeywordsEn{1-Deep Neural Networks, 2-Natural Language Processing, 3- Natural Language Understanding, 4-Intent-Detection, 5-Slot-Filling}