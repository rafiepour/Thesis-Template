% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl
\begin{LTRbibitems}
\resetlatinfont
\bibitem{socialchatbot}
\BIBentryALTinterwordspacing
J.~Lesser, ``From tvs to beertails: How chatbots help brands engage consumers
  on twitter,'' \emph{Twitter Marketing}, Dec 2017. [Online]. Available:
  \url{https://marketing.twitter.com/en/perspectives/
  from-tvs-to-beertails-how-chatbots-help-brands-engage-consumers-on-twitter}

\end{LTRbibitems}
\BIBentrySTDinterwordspacing
\begin{LTRbibitems}
\resetlatinfont
\bibitem{rnn}
D.~Rumelhart, G.~Hinton, and R.~Williams, ``Learning internal representations
  by error propagation,'' California Univ San Diego La Jolla Inst for Cognitive
  Science, Tech. Rep., 1985.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{RNNGradientProblem}
S.~Hochreiter, Y.~Bengio, P.~Frasconi, and J.~Schmidhuber, \emph{Gradient flow
  in recurrent nets: the difficulty of learning long-term dependencies}.\hskip
  1em plus 0.5em minus 0.4em\relax A field guide to dynamical recurrent neural
  networks. IEEE Press, 2001, pp. 237--243.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{VanishingGradient}
Y.~Bengio, P.~Simard, and P.~Frasconi, ``Learning long-term dependencies with
  gradient descent is difficult,'' \emph{IEEE transactions on neural networks},
  vol.~5, no.~2, pp. 157--166, 1994.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{lstm}
S.~Hochreiter and J.~Schmidhuber, ``Long short-term memory,'' \emph{Neural
  computation}, vol.~9, no.~8, pp. 1735--1780, 1997.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{attention_bahdanau}
D.~Bahdanau, K.~Cho, and Y.~Bengio, ``Neural machine translation by jointly
  learning to align and translate,'' \emph{arXiv preprint arXiv:1409.0473},
  2014.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{info12110442}
S.~Noh, ``Analysis of gradient vanishing of rnns and performance comparison,''
  \emph{Information}, vol.~12, no.~11, 2021.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{Kag2020RNNs}
A.~Kag, Z.~Zhang, and V.~Saligrama, ``Rnns incrementally evolving on an
  equilibrium manifold: A panacea for vanishing and exploding gradients?'' in
  \emph{International Conference on Learning Representations}, Addis Ababa,
  Ethiopia, 2020.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{transformerboom}
T.~Wolf, L.~Debut, V.~Sanh, J.~Chaumond, C.~Delangue, A.~Moi \emph{et~al.},
  ``Transformers: State-of-the-art natural language processing,'' in
  \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural
  Language Processing: System Demonstrations}.\hskip 1em plus 0.5em minus
  0.4em\relax Punta Cana, Dominican Republic: Association for Computational
  Linguistics, Oct. 2020, pp. 38--45.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{transformer}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~Gomez
  \emph{et~al.}, ``Attention is all you need,'' in \emph{Proceedings of the
  31st International Conference on Neural Information Processing Systems}, ser.
  NIPS'17.\hskip 1em plus 0.5em minus 0.4em\relax Red Hook, NY, USA: Curran
  Associates Inc., 2017, pp. 6000--6010.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{residualconnection}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in \emph{Proceedings of the IEEE conference on computer vision
  and pattern recognition}, Las Vegas, United States, 2016, pp. 770--778.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{word2vec}
T.~Mikolov, K.~Chen, G.~Corrado, and J.~Dean, ``Efficient estimation of word
  representations in vector space,'' \emph{arXiv preprint arXiv:1301.3781},
  2013.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{glove}
J.~Pennington, R.~Socher, and C.~Manning, ``{G}lo{V}e: Global vectors for word
  representation,'' in \emph{Proceedings of the 2014 Conference on Empirical
  Methods in Natural Language Processing ({EMNLP})}.\hskip 1em plus 0.5em minus
  0.4em\relax Doha, Qatar: Association for Computational Linguistics, Oct.
  2014, pp. 1532--1543.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{elmo}
M.~Peters, M.~Neumann, M.~Iyyer, M.~Gardner, C.~Clark, K.~Lee \emph{et~al.},
  ``Deep contextualized word representations,'' in \emph{Proceedings of the
  2018 Conference of the North {A}merican Chapter of the Association for
  Computational Linguistics: Human Language Technologies, Volume 1 (Long
  Papers)}.\hskip 1em plus 0.5em minus 0.4em\relax New Orleans, Louisiana:
  Association for Computational Linguistics, Jun. 2018, pp. 2227--2237.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{bert}
J.~Devlin, M.~Chang, K.~Lee, and K.~Toutanova, ``{BERT}: Pre-training of deep
  bidirectional transformers for language understanding,'' in \emph{Proceedings
  of the 2019 Conference of the North {A}merican Chapter of the Association for
  Computational Linguistics: Human Language Technologies, Volume 1 (Long and
  Short Papers)}.\hskip 1em plus 0.5em minus 0.4em\relax Minneapolis, USA:
  Association for Computational Linguistics, Jun. 2019, pp. 4171--4186.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{louvan2018exploring}
S.~Louvan and B.~Magnini, ``Exploring named entity recognition as an auxiliary
  task for slot filling in conversational language understanding,'' in
  \emph{Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International
  Workshop on Search-Oriented Conversational AI}, Brussels, Belgium, 2018, pp.
  74--80.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{vu2016bi}
N.~T. Vu, P.~Gupta, H.~Adel, and H.~Sch{\"u}tze, ``Bi-directional recurrent
  neural network with ranking loss for spoken language understanding,'' in
  \emph{2016 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax Shanghai, China:
  IEEE, 2016, pp. 6060--6064.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{aligned_lstm_atten_nlu}
B.~Liu and I.~Lane, ``Attention-based recurrent neural network models for joint
  intent detection and slot filling,'' in \emph{Proceedings of Interspeech
  2016}, San Francisco, USA, 2016, pp. 685--689.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{Wang:18}
Y.~Wang, L.~Tang, and T.~He, ``Attention-based cnn-blstm networks for joint
  intent detection and slot filling,'' in \emph{Chinese Computational
  Linguistics and Natural Language Processing Based on Naturally Annotated Big
  Data}, M.~Sun, T.~Liu, X.~Wang, Z.~Liu, and Y.~Liu, Eds.\hskip 1em plus 0.5em
  minus 0.4em\relax Cham, Switzerland: Springer International Publishing, May
  2018, pp. 250--261.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{goo-etal-2018-slot}
C.~Goo, G.~Gao, Y.~Hsu, C.~Huo, T.~Chen, K.~Hsu \emph{et~al.}, ``Slot-gated
  modeling for joint slot filling and intent prediction,'' in \emph{Proceedings
  of the 2018 Conference of the North {A}merican Chapter of the Association for
  Computational Linguistics: Human Language Technologies, Volume 2 (Short
  Papers)}.\hskip 1em plus 0.5em minus 0.4em\relax New Orleans, USA:
  Association for Computational Linguistics, Jun. 2018, pp. 753--757.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{zhang-2018-joint}
Y.~Zhang, ``Joint models for {NLP},'' in \emph{Proceedings of the 2018
  Conference on Empirical Methods in Natural Language Processing: Tutorial
  Abstracts}.\hskip 1em plus 0.5em minus 0.4em\relax Melbourne, Australia:
  Association for Computational Linguistics, Oct.-Nov. 2018.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{jaech2016domain}
A.~Jaech, L.~Heck, and M.~Ostendorf, ``Domain adaptation of recurrent neural
  networks for natural language understanding,'' \emph{arXiv preprint
  arXiv:1604.00117}, 2016.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{wei2022joint}
P.~Wei, B.~Zeng, and W.~Liao, ``Joint intent detection and slot filling with
  wheel-graph attention networks,'' \emph{Journal of Intelligent \& Fuzzy
  Systems}, vol.~42, no.~3, pp. 2409--2420, 2022.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{varghese2020bidirectional}
S.~Varghese, S.~Sarang, V.~Yadav, B.~Karotra, and N.~Gandhi, ``Bidirectional
  lstm joint model for intent classification and named entity recognition in
  natural language understanding,'' \emph{International Journal of Hybrid
  Intelligent Systems}, vol.~16, no.~1, pp. 13--23, 2020.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{xu2020end}
W.~Xu, B.~Haider, and S.~Mansour, ``End-to-end slot alignment and recognition
  for cross-lingual nlu,'' \emph{arXiv preprint arXiv:2004.14353}, 2020.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{e:2019}
H.~E, P.~Niu, Z.~Chen, and M.~Song, ``A novel bi-directional interrelated model
  for joint intent detection and slot filling,'' in \emph{Proceedings of the
  57th Annual Meeting of the Association for Computational Linguistics}.\hskip
  1em plus 0.5em minus 0.4em\relax Florence, Italy: Association for
  Computational Linguistics, Jul. 2019, pp. 5467--5471.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{Firdaus:2021}
M.~Firdaus, H.~Golchha, A.~Ekbal, and P.~Bhattacharyya, ``A deep multi-task
  model for dialogue act classification, intent detection and slot filling,''
  \emph{Cognitive Computation}, vol.~13, no.~3, pp. 626--645, May 2021.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{priorknowledge}
C.~Hou, J.~Li, H.~Yu, X.~Luo, and S.~Xie, \emph{Prior knowledge modeling for
  joint intent detection and slot filling}.\hskip 1em plus 0.5em minus
  0.4em\relax World Scientific, 2023, pp. 3--10.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{chen:2019}
Q.~Chen, Z.~Zhuo, and W.~Wang, ``Bert for joint intent classification and slot
  filling,'' \emph{arXiv preprint arXiv:1902.10909}, 2019.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{Wang:2020}
C.~Wang, Z.~Huang, and M.~Hu, ``Sasgbc: Improving sequence labeling performance
  for joint learning of slot filling and intent detection,'' in
  \emph{Proceedings of 2020 the 6th International Conference on Computing and
  Data Engineering}, ser. ICCDE 2020.\hskip 1em plus 0.5em minus 0.4em\relax
  New York, USA: Association for Computing Machinery, 2020, p. 29–33.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{huang:2020}
Z.~Huang, F.~Liu, and Y.~Zou, ``Federated learning for spoken language
  understanding,'' in \emph{Proceedings of the 28th International Conference on
  Computational Linguistics}.\hskip 1em plus 0.5em minus 0.4em\relax Barcelona,
  Spain: International Committee on Computational Linguistics, Dec. 2020, pp.
  3467--3478.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{Qin:2021}
L.~Qin, T.~Liu, W.~Che, B.~Kang, S.~Zhao, and T.~Liu, ``A co-interactive
  transformer for joint slot filling and intent detection,'' in \emph{2021 -
  2021 IEEE International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}, Toronto, Canada, 2021, pp. 8193--8197.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{yang:2021}
P.~Yang, D.~Ji, C.~Ai, and B.~Li, ``Aise: Attending to intent and slots
  explicitly for better spoken language understanding,'' \emph{Knowledge-Based
  Systems}, vol. 211, p. 106537, 2021.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{relative_positioning_transformer}
Z.~Dai, Z.~Yang, Y.~Yang, J.~Carbonell, Q.~Le, and R.~Salakhutdinov,
  ``Transformer-xl: Attentive language models beyond a fixed-length context,''
  \emph{arXiv preprint arXiv:1901.02860}, 2019.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{Siddhant:2019}
A.~Siddhant, A.~Goyal, and A.~Metallinou, ``Unsupervised transfer learning for
  spoken language understanding in intelligent agents,'' \emph{Proceedings of
  the Association for the Advancement of Artificial Intelligence Conference on
  Artificial Intelligence}, vol.~33, no.~01, pp. 4959--4966, jul 2019.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{ethayarajh-2019-contextual}
K.~Ethayarajh, ``How contextual are contextualized word representations?
  {C}omparing the geometry of {BERT}, {ELM}o, and {GPT}-2 embeddings,'' in
  \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural
  Language Processing and the 9th International Joint Conference on Natural
  Language Processing (EMNLP-IJCNLP)}.\hskip 1em plus 0.5em minus 0.4em\relax
  Hong Kong, China: Association for Computational Linguistics, Nov. 2019, pp.
  55--65.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{wordpiece}
Y.~Wu, M.~Schuster, Z.~Chen, Q.~Le, M.~Norouzi, W.~Macherey \emph{et~al.},
  ``Google's neural machine translation system: Bridging the gap between human
  and machine translation,'' \emph{arXiv preprint arXiv:1609.08144}, 2016.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{zhou:15clstm}
C.~Zhou, C.~Sun, Z.~Liu, and F.~Lau, ``A c-lstm neural network for text
  classification,'' \emph{arXiv preprint arXiv:1511.08630}, 2015.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{ngram}
C.~Suen, ``n-gram statistics for natural language understanding and text
  processing,'' \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, vol. PAMI-1, no.~2, pp. 164--172, 1979.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{5700816}
G.~Tur, D.~HakkaniTür, and L.~Heck, ``What is left to be understood in atis?''
  in \emph{2010 IEEE Spoken Language Technology Workshop}, Berkeley, USA, 2010,
  pp. 19--24.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{atis:hemphill1990}
C.~Hemphill, J.~Godfrey, and G.~Doddington, ``The {ATIS} spoken language
  systems pilot corpus,'' in \emph{Speech and Natural Language: Proceedings of
  a Workshop}, Hidden Valley, USA, June 1990.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{snips:coucke2018}
A.~Coucke, A.~Saade, A.~Ball, T.~Bluche, A.~Caulier, D.~Leroy \emph{et~al.},
  ``Snips voice platform: an embedded spoken language understanding system for
  private-by-design voice interfaces,'' \emph{arXiv preprint arXiv:1805.10190},
  2018.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{adamw:Loshchilov}
I.~Loshchilov and F.~Hutter, ``Decoupled weight decay regularization,''
  \emph{arXiv preprint arXiv:1711.05101}, 2017.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{dropout:Srivastava}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov,
  ``Dropout: A simple way to prevent neural networks from overfitting,''
  \emph{Journal of Machine Learning Research}, vol.~15, no.~1, pp. 1929--1958,
  jan 2014.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{clipping:mikolov}
R.~Pascanu, T.~Mikolov, and Y.~Bengio, ``On the difficulty of training
  recurrent neural networks,'' in \emph{Proceedings of the 30th International
  Conference on International Conference on Machine Learning - Volume 28}, ser.
  ICML'13.\hskip 1em plus 0.5em minus 0.4em\relax Microtome Publishing, 2013,
  pp. 1310--1318.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{Firdaus:2019}
M.~Firdaus, A.~Kumar, A.~Ekbal, and P.~Bhattacharyya, ``A multi-task
  hierarchical approach for intent detection and slot filling,''
  \emph{Knowledge-Based Systems}, vol. 183, 2019.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{kane:2020}
B.~Kane, F.~Rossi, O.~Guinaudeau, V.~Chiesa, I.~Quénel, and S.~Chau, ``Joint
  intent detection and slot filling via cnn-lstm-crf,'' in \emph{2020 6th IEEE
  Congress on Information Science and Technology (CiSt)}, Virtual Event, 2020,
  pp. 342--347.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{liu:2019}
Y.~Liu, F.~Meng, J.~Zhang, J.~Zhou, Y.~Chen, and J.~Xu, ``{CM}-net: A novel
  collaborative memory network for spoken language understanding,'' in
  \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural
  Language Processing and the 9th International Joint Conference on Natural
  Language Processing (EMNLP-IJCNLP)}.\hskip 1em plus 0.5em minus 0.4em\relax
  Hong Kong, China: Association for Computational Linguistics, Nov. 2019, pp.
  1051--1060.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{Tang:2020}
H.~Tang, D.~Ji, and Q.~Zhou, ``End-to-end masked graph-based crf for joint slot
  filling and intent detection,'' \emph{Neurocomputing}, vol. 413, pp.
  348--359, 2020.

\end{LTRbibitems}
\begin{LTRbibitems}
\resetlatinfont
\bibitem{doubledescent}
P.~Nakkiran, G.~Kaplun, Y.~Bansal, T.~Yang, B.~Barak, and I.~Sutskever, ``Deep
  double descent: Where bigger models and more data hurt,'' \emph{Journal of
  Statistical Mechanics: Theory and Experiment}, vol. 2021, no.~12, p. 124003,
  2021.

\end{LTRbibitems}

\end{thebibliography}
